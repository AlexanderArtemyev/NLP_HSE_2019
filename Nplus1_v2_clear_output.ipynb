{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S3PRcm7NH2yw"
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6Qce3PNGupI"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0FVzekCPG1eH"
   },
   "outputs": [],
   "source": [
    "url = 'https://nplus1.ru/'\n",
    "url = 'https://nplus1.ru/news/'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VneF9jRFn9On"
   },
   "outputs": [],
   "source": [
    "url = 'https://postnauka.ru/?page=4'\n",
    "page = requests.get(url)\n",
    "soup_p = BeautifulSoup(page.text, 'lxml')\n",
    "# print(soup_p.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "RX7ddDyNHN3E",
    "outputId": "455b8d4f-90cb-4b53-a9f6-6f7ab2870137"
   },
   "outputs": [],
   "source": [
    "# Подключаем google disk\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "LCbjkQ5hOlug",
    "outputId": "d3b28b02-8678-4749-c7ea-7b22b0fedd5a"
   },
   "outputs": [],
   "source": [
    "!ls '/content/drive/My Drive/data/df_Nplus1.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RyIfG9lXHw4g"
   },
   "source": [
    "# Getting (downloading) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JGFboK9GHdMa",
    "outputId": "5efb9855-e614-4e36-b2cc-9660e7f999b2"
   },
   "outputs": [],
   "source": [
    "def get_links_from_soup(soup):\n",
    "    urls = []\n",
    "    for link in soup.find_all('a'):\n",
    "        if '/news/' in link.get('href'):\n",
    "            urls.append(link.get('href'))\n",
    "            \n",
    "    full_urls = []\n",
    "    for u in urls:\n",
    "        res = 'https://nplus1.ru' + u\n",
    "        full_urls.append(res) \n",
    "\n",
    "    return(full_urls)\n",
    "\n",
    "full_urls = get_links_from_soup(soup)\n",
    "full_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LNsNC4zTGIiH"
   },
   "outputs": [],
   "source": [
    "# https://nbviewer.jupyter.org/github/allatambov/py-icef/blob/master/5-02-04/web-scrape.ipynb\n",
    "\n",
    "def GetNews(url0):\n",
    "    \"\"\"\n",
    "    Returns a tuple with url0, date, author, description, title, final_text, rubrics, diff.\n",
    "    Parameters:\n",
    "    \n",
    "    url0 is a link to the news (string)\n",
    "    \"\"\"\n",
    "    page0 = requests.get(url0)\n",
    "    soup0 = BeautifulSoup(page0.text, 'lxml')\n",
    "    author = soup0.find_all('meta', {'name' : 'author'})[0].attrs['content']\n",
    "    date = soup0.find_all('meta', {'itemprop' : 'datePublished'})[0].attrs['content']\n",
    "    title = soup0.find_all('meta', {'property' : 'og:title'})[0].attrs['content']\n",
    "    description = soup0.find_all('meta', {'name' : 'description'})[0].attrs['content']\n",
    "    #rubrics = [r.text for r in soup0.find_all('p')[0].find_all('a')]\n",
    "    #diff = soup0.find_all('span', {'class' : 'difficult-value'})[0].text\n",
    "    text_list = soup0.find_all('p', {'class' : None})\n",
    "    text_list_2 = [txt_lst.text for txt_lst in text_list\n",
    "                          if (txt_lst.text not in [\n",
    "                                             'Нашли опечатку? Выделите фрагмент и нажмите Ctrl+Enter.', \n",
    "                                             'Коэффициент сложности',\n",
    "                                             '© 2019 N+1 Интернет-издание \\xa0 Свидетельство о регистрации СМИ Эл № ФС77-67614',\n",
    "                                             'Использование всех текстовых материалов без изменений в некоммерческих целях разрешается со ссылкой на N+1. \\n                        Все аудиовизуальные произведения являются собственностью своих авторов и правообладателей и используются \\n                        только в образовательных и информационных целях. Если вы являетесь собственником того или иного произведения \\n                        и не согласны с его размещением на нашем сайте, пожалуйста, напишите на kirill@nplus1.ru',\n",
    "                                             'Материалы, опубликованные в разделе «Блоги», отражают позиции их авторов, которые могут не совпадать с мнением редакции.',\n",
    "                                             'Сайт может содержать контент, не предназначенный для лиц младше 18 лет.',\n",
    "                                             '\\nПолитика обработки персональных данных пользователей сайта\\n',\n",
    "                                             '\\n\\nChange privacy settings\\n',\n",
    "                                             '\\n',\n",
    "                                             '',\n",
    "                                             author,\n",
    "                                             author + '.'\n",
    "                                            ] )\n",
    "                  ]\n",
    "    text = [txt_lst.replace('\\xa0', ' ') for txt_lst in text_list_2]\n",
    "    #text = [t.text for t in text_list]\n",
    "    \n",
    "\n",
    "    final_text = ' '.join(text)\n",
    "    final_text = final_text.replace('\\n', ' ')\n",
    "    \n",
    "    #return url0, date, author, description, title, final_text, rubrics, diff\n",
    "    return url0, date, author, description, title, final_text #, rubrics, diff\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "leXTHb8uNFcl",
    "outputId": "2c6524df-a7da-4aab-84c3-42236445c9cf"
   },
   "outputs": [],
   "source": [
    "news = [] # это будет список из кортежей, в которых будут храниться данные по каждой новости\n",
    "\n",
    "N_news = 500\n",
    "i_news = 0\n",
    "for link in full_urls:\n",
    "    i_news = i_news + 1\n",
    "    print(i_news, link)\n",
    "    res = GetNews(link)\n",
    "    news.append(res)\n",
    "    time_sleep = randrange(30,90) / 10.\n",
    "    #print(time_sleep)\n",
    "    sleep(time_sleep)\n",
    "    \n",
    "    df = pd.DataFrame(news, columns = ['url', 'date', 'author', 'description', 'title', 'final_text'])\n",
    "    path_data = '/content/drive/My Drive/data/'\n",
    "    df.to_excel(path_data + 'df_Nplus1.xlsx', index = False) \n",
    "\n",
    "    if i_news > N_news:\n",
    "        break\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "colab_type": "code",
    "id": "q8xPwPA93AZT",
    "outputId": "ee218f62-7ad0-4d26-e7dc-48768901d4ed"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "EbbsJIP6OSaA",
    "outputId": "ecf3e2cd-938e-435f-dcc6-1f0d9046d090"
   },
   "outputs": [],
   "source": [
    "!ls '/content/drive/My Drive/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "btcd_eBN3AAW"
   },
   "source": [
    "# Reading saved data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "jlRw6APkIECa",
    "outputId": "ff0b590e-5f6b-42c8-ad5d-2ab689fd0389"
   },
   "outputs": [],
   "source": [
    "path_data = '/content/drive/My Drive/data/'\n",
    "file_name = path_data + 'df_Nplus1.xlsx'\n",
    "print(file_name)\n",
    "df = pd.read_excel(file_name)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFasYlhKIGbv"
   },
   "source": [
    "# Analyzing text\n",
    "* https://github.com/oserikov/data-science-nlp\n",
    "* https://github.com/oserikov/data-science-nlp/blob/master/1.%20Intro.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "_CH9aO-UCNHx",
    "outputId": "3231fc80-16de-4a95-dc55-0de7bc96185f"
   },
   "outputs": [],
   "source": [
    "!pip -qq install yargy --progress-bar off\n",
    "!pip -qq install pymorphy2 --progress-bar off\n",
    "!pip -qq install rusenttokenize --progress-bar off\n",
    "import nltk\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2pDx5ABI5IVV"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "#from yargy.tokenizer import MorphTokenizer\n",
    "from nltk.tokenize import word_tokenize, ToktokTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize  import sent_tokenize\n",
    "from rusenttokenize import ru_sent_tokenize\n",
    "from nltk.tokenize import word_tokenize, ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "from pymorphy2 import MorphAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e7199zYZC6lf"
   },
   "outputs": [],
   "source": [
    "# wordcloud stuff\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def show_wordcloud_by_freq_dict(freq_dict):\n",
    "    wordcloud = WordCloud(background_color='white',\n",
    "                          collocations=False,\n",
    "                          width=800,\n",
    "                          height=400,\n",
    "                          min_font_size=8,\n",
    "                          max_font_size=100,\n",
    "                         )\n",
    "    wordcloud.generate_from_frequencies(frequencies=freq_dict)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(path_data + author +  '.png', dpi = 600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAm8O48pitNc"
   },
   "outputs": [],
   "source": [
    "?WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYTuC6DIXZec"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#print(SnowballStemmer.languages)\n",
    "snowball = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lPJi5x7jWRng"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict as dd\n",
    "from operator import itemgetter\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from rusenttokenize import ru_sent_tokenize\n",
    "import string\n",
    "import pymorphy2\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#print(SnowballStemmer.languages)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORh6ydr2DZFP"
   },
   "outputs": [],
   "source": [
    "morph_analyzer = pymorphy2.MorphAnalyzer()\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "\n",
    "def preprocess_tokenize(text):\n",
    "    \n",
    "    text_preprocessed_tokenized = []\n",
    "        \n",
    "    for sentence in ru_sent_tokenize(text):\n",
    "        \n",
    "        clean_words = [word.strip(string.punctuation) for word in word_tokenize(text)]\n",
    "        clean_words = [word for word in clean_words if word]\n",
    "        clean_words = [word.lower() for word in clean_words if word]\n",
    "        clean_words = [word for word in clean_words if word not in russian_stopwords]\n",
    "        \n",
    "        clean_lemmas = [morph_analyzer.parse(word)[0].normal_form for word in clean_words]\n",
    "        #clean_lemmas = [snowball.stem(word) for word in clean_words]\n",
    "        \n",
    "        text_preprocessed_tokenized.extend(clean_lemmas)\n",
    "\n",
    "    return text_preprocessed_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TlIeOJzaDi6h",
    "outputId": "cfbfd3ba-64fe-445a-f325-56453a6b72b5"
   },
   "outputs": [],
   "source": [
    "preprocess_tokenize(df.final_text[0][:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "oFhQYWE5absm",
    "outputId": "fa13998a-e5b3-4986-9628-496bc95b43d3"
   },
   "outputs": [],
   "source": [
    "!ls \"$path_data\"\n",
    "#'/content/drive/My Drive/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "5Bn_IJR86b28",
    "outputId": "573c9799-17e3-4700-dbf0-6a08038e53bf"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df['tokens_list'] = df.final_text.map(preprocess_tokenize)\n",
    "file_name = '/content/drive/My Drive/data/df_tokens_list_Nplus1.xlsx'\n",
    "print(file_name)\n",
    "#df.to_excel(file_name, index=False)\n",
    "#df = pd.read_excel(file_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "B1POPbLuINch",
    "outputId": "7167297f-1e5e-4425-84dc-f86a1dea8094"
   },
   "outputs": [],
   "source": [
    "print(df.author.value_counts().head(12))\n",
    "top_authors = list(df.author.value_counts().head(8).index)\n",
    "top_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "ChB6AexfFziN",
    "outputId": "4f7995a9-3766-4fbc-c8a7-2b9e543887b5"
   },
   "outputs": [],
   "source": [
    "def get_text_author(author, df):\n",
    "    text_author = ' '.join(list(df.final_text[df.author == author].values))\n",
    "    return text_author\n",
    "  \n",
    "def get_tokenized_text_author(author, df):\n",
    "    list_of_lists = list(df.tokens_list[df.author == author].values)\n",
    "    flat_list = [item for sublist in list_of_lists for item in sublist]\n",
    "    return flat_list\n",
    "    \n",
    "texts_by_top_authors = []\n",
    "tokenized_texts_by_top_authors = []\n",
    "\n",
    "for author in top_authors:\n",
    "    print(author)\n",
    "    texts_by_top_authors.append(get_text_author(author, df))\n",
    "    tokenized_texts_by_top_authors.append(get_tokenized_text_author(author, df))\n",
    "\n",
    "df_author = pd.DataFrame(zip(top_authors,\n",
    "                             texts_by_top_authors,\n",
    "                             tokenized_texts_by_top_authors\n",
    "                            ), columns = ['author', 'text', 'tokenized_text'])\n",
    "df_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 964
    },
    "colab_type": "code",
    "id": "PJ4jpTKJX_k2",
    "outputId": "4ccb80ea-ce5e-4249-f339-5939ad30b3a1"
   },
   "outputs": [],
   "source": [
    "df.tokens_list[df.author == 'Василий Сычев'].values\n",
    "#get_tokenized_text_author(, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jE5TaJG5X95q"
   },
   "outputs": [],
   "source": [
    "corpus = df_author['texts'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ri8Iipd5QLM1",
    "outputId": "d184a6bd-6108-4ac2-f459-189694d6ae09"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "corpus = df_author['text'].values\n",
    "len(corpus)\n",
    "\n",
    "tokenized_sentences = list(df_author['tokenized_text'])\n",
    "type(tokenized_sentences[0])\n",
    "\n",
    "#tfidf_vectorizer = TfidfVectorizer(tokenizer=preprocess_tokenize)\n",
    "#tfidf_vectorizer.fit_transform(corpus)\n",
    "#feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=' '.join)\n",
    "tfidf_vectorizer.fit_transform(list(df_author['tokenized_text']))\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "#tfidf = TfidfVectorizer()\n",
    "#tfidf.fit_transform(corpus)\n",
    "\n",
    "#tokenized_sentences = [['this', 'is', 'one', 'cat', 'or', 'dog'],\n",
    "#                       ['this', 'is', 'another', 'dog']]\n",
    "\n",
    "\n",
    "#tfidf = TfidfVectorizer(tokenizer=lambda x: x,\n",
    "#                        preprocessor=lambda x: x, stop_words='russian')\n",
    "#tfidf.fit_transform(# tokenized_sentences\n",
    "#                   corpus\n",
    "#                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4EYo0kuiPHbw",
    "outputId": "7ef209df-b39e-4ad6-947d-4e47f50e8520"
   },
   "outputs": [],
   "source": [
    "for index, row in df_author.iterrows():\n",
    "    author = row.author\n",
    "    document = row.text\n",
    "    tokenized_document = row.tokenized_text\n",
    "    print(author)\n",
    "    X = tfidf_vectorizer.transform([tokenized_document])\n",
    "    \n",
    "    tfidf_scores = [(feature_names[col], X[0, col]) for col in X.nonzero()[1]]\n",
    "    freq_list = [(word, freq) for word, freq in sorted( tfidf_scores, \n",
    "                                                        key=itemgetter(1), \n",
    "                                                        reverse=True)]\n",
    "\n",
    "    show_wordcloud_by_freq_dict(dict(freq_list))\n",
    "    #print(document)\n",
    "    print(\"\")\n",
    "\n",
    "    #proceed = input(\"proceed? ( [n] to refuse) : \")\n",
    "    #if proceed.strip().lower() == \"n\":\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "boP5i1y2THvO"
   },
   "source": [
    "### Получилось облако слов для каждого автора.\n",
    "Можно надеяться увидеть любимые слова авторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLeNiUJATcTy"
   },
   "source": [
    "## Закон Ципфа для разных авторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LeuEnt4STcqw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8yWvz3JTczP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3RUn_avJTc7M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OR87uswATc9n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WWRJo_gDTdAq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2-tPg_cCTdDi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Nplus1_v2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
